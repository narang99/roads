{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a8f9d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30c71a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "542c80b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PROJECT_CODE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m DS = Path(\u001b[33m\"\u001b[39m\u001b[33m../../datasets/\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m BASE_DS_DIR = DS / \u001b[33m\"\u001b[39m\u001b[33mT004-taco-crops\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m EXP_BASE = BASE_DS_DIR / \u001b[43mPROJECT_CODE\u001b[49m\n\u001b[32m      6\u001b[39m OUTS = BASE_DS_DIR / \u001b[33m\"\u001b[39m\u001b[33msynth\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m LOG_BASE = EXP_BASE / \u001b[33m\"\u001b[39m\u001b[33mlog\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'PROJECT_CODE' is not defined"
     ]
    }
   ],
   "source": [
    "NUM_SAMPLES = 5000\n",
    "\n",
    "DS = Path(\"../../datasets/\")\n",
    "BASE_DS_DIR = DS / \"T004-taco-crops\"\n",
    "EXP_BASE = BASE_DS_DIR / PROJECT_CODE\n",
    "OUTS = BASE_DS_DIR / \"synth\"\n",
    "LOG_BASE = EXP_BASE / \"log\"\n",
    "TACO_BASE_DIR = Path(\"/Users/hariomnarang/Desktop/personal/TACO/data/\")\n",
    "ANN_FILE = TACO_BASE_DIR / \"annotations.json\"\n",
    "TEST_BIG_IMG = BASE_DS_DIR / \"14325.jpeg\"\n",
    "\n",
    "LOG_BASE.mkdir(parents=True, exist_ok=True)\n",
    "DS.exists(), TACO_BASE_DIR.exists(), ANN_FILE.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "089be1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'006'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# im assuming hte model is good, we only rename whatever is done\n",
    "\n",
    "def _is_numeric_dir(d):\n",
    "    try:\n",
    "        int(d.name)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def _get_max_num_dir():\n",
    "    r = []\n",
    "    for d in BASE_DS_DIR.glob(\"*\"):\n",
    "        if not d.is_dir():\n",
    "            continue\n",
    "        if _is_numeric_dir(d):\n",
    "            r.append(int(d.name))\n",
    "    if not r:\n",
    "        return 1\n",
    "    return max(r)\n",
    "\n",
    "def _num_to_dir(n):\n",
    "    return f\"{n:03d}\"\n",
    "\n",
    "def _next_dir_name():\n",
    "    existing_max = _get_max_num_dir()\n",
    "    return _num_to_dir(existing_max + 1)\n",
    "\n",
    "_next_dir_name()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3040e27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import json\n",
    "import csv\n",
    "from typing import Optional\n",
    "\n",
    "def _dict_to_markdown_table(d):\n",
    "    keys = list(d.keys())\n",
    "    vals = list(d.values())\n",
    "\n",
    "    max_key = max(len(str(k)) for k in keys + [\"Key\"])\n",
    "    max_val = max(len(str(v)) for v in vals + [\"Value\"])\n",
    "\n",
    "    lines = []\n",
    "    lines.append(f\"| {'Key'.ljust(max_key)} | {'Value'.ljust(max_val)} |\")\n",
    "    lines.append(f\"|{'-' * (max_key + 2)}|{'-' * (max_val + 2)}|\")\n",
    "\n",
    "    for k, v in d.items():\n",
    "        lines.append(f\"| {str(k).ljust(max_key)} | {str(v).ljust(max_val)} |\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "class ParsedDir:\n",
    "    def __init__(\n",
    "        self, orig_path, params\n",
    "    ):\n",
    "        self.orig_path = orig_path\n",
    "        self.params = params\n",
    "\n",
    "    @classmethod\n",
    "    def parse(cls, dir_path) -> \"ParsedDir\":\n",
    "        res = {}\n",
    "        parts = dir_path.name.split(\"-\")\n",
    "        for part in parts:\n",
    "            try:\n",
    "                k, v = part.split(\"=\")\n",
    "            except Exception as ex:\n",
    "                raise Exception(f\"part: {part}\") from ex\n",
    "            res[k] = v\n",
    "        return cls(dir_path, res)\n",
    "    \n",
    "    def get_img_size(self) -> Optional[int]:\n",
    "        try:\n",
    "            return int(self.params[\"FILE_SIZE\"])\n",
    "        except Exception as ex:\n",
    "            print(f\"could not get image size for {self.orig_path} reason={ex}\")\n",
    "            return None\n",
    "\n",
    "    @property\n",
    "    def export_pkl(self):\n",
    "        return self.orig_path / \"log\" / \"export.pkl\"\n",
    "\n",
    "    @property\n",
    "    def history(self):\n",
    "        return self.orig_path / \"log\" / \"history.csv\"\n",
    "\n",
    "    @property\n",
    "    def result_png(self):\n",
    "        return self.orig_path / \"res.png\"\n",
    "\n",
    "    def dump(self, new_dir, dry_run=True):\n",
    "        dest = self.orig_path / \"README.md\"\n",
    "        content = self._get_content_readme()\n",
    "        print(content)\n",
    "        print(f\"writing to {dest}\")\n",
    "        if not dry_run:\n",
    "            self._dump_params(self.orig_path / \"params.json\")\n",
    "            with open(dest, \"a\") as f:\n",
    "                f.write(content)\n",
    "\n",
    "        new_dir = Path(new_dir)\n",
    "        print(f\"Move: {self.orig_path} -> {new_dir}\")\n",
    "        if not dry_run:\n",
    "            shutil.move(self.orig_path, new_dir)\n",
    "\n",
    "    def _get_content_readme(self):\n",
    "        content = \"\\n# Params\\n\\n\"\n",
    "        content += _dict_to_markdown_table(self.params)\n",
    "    \n",
    "        if self.history.exists():\n",
    "            content += \"\\n\\n---\"\n",
    "            content += \"\\n\\n# History\"\n",
    "            content += \"\\n\\n\"\n",
    "            content += csv_to_markdown(self.history)\n",
    "            content += \"\\n\\n---\"\n",
    "        return content\n",
    "\n",
    "    def _dump_params(self, path):\n",
    "        with open(path, \"w\")as f:\n",
    "            json.dump(self.params, f)\n",
    "\n",
    "def csv_to_markdown(path):\n",
    "    with open(path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "        rows = list(csv.reader(f))\n",
    "\n",
    "    if not rows:\n",
    "        return \"\"\n",
    "\n",
    "    cols = len(rows[0])\n",
    "    widths = [\n",
    "        max(len(row[i]) if i < len(row) else 0 for row in rows)\n",
    "        for i in range(cols)\n",
    "    ]\n",
    "\n",
    "    def fmt(row):\n",
    "        return \"| \" + \" | \".join(\n",
    "            (row[i] if i < len(row) else \"\").ljust(widths[i])\n",
    "            for i in range(cols)\n",
    "        ) + \" |\"\n",
    "\n",
    "    header = fmt(rows[0])\n",
    "    sep = \"| \" + \" | \".join(\"-\" * w for w in widths) + \" |\"\n",
    "    body = \"\\n\".join(fmt(r) for r in rows[1:])\n",
    "\n",
    "    return \"\\n\".join([header, sep, body])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f7cbde77",
   "metadata": {},
   "outputs": [],
   "source": [
    "drs = filter(Path.is_dir, BASE_DS_DIR.glob(\"*\"))\n",
    "drs = filter(lambda d: not _is_numeric_dir(d), drs)\n",
    "drs = filter(lambda d: d.name.startswith(\"FINE_TUNE\"), drs)\n",
    "drs = map(ParsedDir.parse, drs)\n",
    "drs = filter(lambda dr: dr.export_pkl.exists(), drs)\n",
    "drs = list(drs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2bc4a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(drs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "97054ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create res png if it does not exist\n",
    "from tqdm import tqdm\n",
    "from mtrain.smallnet.predict import tile_image_and_predict\n",
    "from fastai.vision.all import load_learner\n",
    "\n",
    "def make_res_pngs(drs, dry_run=True):\n",
    "    for dr in tqdm(drs):\n",
    "        if dr.result_png.exists():\n",
    "            continue\n",
    "        sz = dr.get_img_size()\n",
    "        if dry_run:\n",
    "            print(f\"run inference: {dr.orig_path} size={sz}\")\n",
    "        else:\n",
    "            learn = load_learner(dr.export_pkl)\n",
    "            if sz:\n",
    "                res = tile_image_and_predict(TEST_BIG_IMG, learn, sz)\n",
    "                plt.imsave(dr.result_png, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a3d0805a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:00<00:00, 54874.60it/s]\n"
     ]
    }
   ],
   "source": [
    "make_res_pngs(drs, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ac0dd72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dr in drs:\n",
    "    if not dr.orig_path.exists():\n",
    "        # already moved\n",
    "        continue\n",
    "        \n",
    "    if dr.result_png.exists() and dr.export_pkl.exists() and dr.history.exists():\n",
    "        next_dir = BASE_DS_DIR / _next_dir_name()\n",
    "        dr.dump(next_dir, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bc9eeca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class DumpedDir:\n",
    "    def __init__(self, d: Path, params: dict):\n",
    "        self.d = d\n",
    "        self.params = params\n",
    "\n",
    "    def dump_params(self):\n",
    "        with open(self.d / \"params.json\", \"w\") as f:\n",
    "            json.dump(self.params, f)\n",
    "\n",
    "    @classmethod\n",
    "    def parse_markdown(cls, d: Path):\n",
    "        mkd = d / \"README.md\"\n",
    "        if not mkd.exists():\n",
    "            return None\n",
    "        with open(mkd) as f:\n",
    "            params, _ = parse_markdown_params_and_history(f.read())\n",
    "        return cls(d, params)\n",
    "        \n",
    "def parse_markdown_params_and_history(md):\n",
    "    def parse_table(lines):\n",
    "        header = [h.strip() for h in lines[0].strip(\"|\").split(\"|\")]\n",
    "        rows = []\n",
    "        for line in lines[2:]:  # skip header + separator\n",
    "            vals = [v.strip() for v in line.strip(\"|\").split(\"|\")]\n",
    "            rows.append(dict(zip(header, vals)))\n",
    "        return rows\n",
    "\n",
    "    lines = [l.rstrip() for l in md.splitlines()]\n",
    "    tables = []\n",
    "    current = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(\"|\"):\n",
    "            current.append(line)\n",
    "        else:\n",
    "            if current:\n",
    "                tables.append(current)\n",
    "                current = []\n",
    "    if current:\n",
    "        tables.append(current)\n",
    "\n",
    "    params_rows = parse_table(tables[0])\n",
    "    history_rows = parse_table(tables[1])\n",
    "\n",
    "    params = {row[\"Key\"]: row[\"Value\"] for row in params_rows}\n",
    "\n",
    "    return params, history_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c6c1039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dumps = BASE_DS_DIR.glob(\"*\")\n",
    "dumps = filter(Path.is_dir, dumps)\n",
    "dumps = filter(_is_numeric_dir, dumps)\n",
    "dumps = filter(lambda d: int(d.name) >= 6, dumps)\n",
    "dumps = map(DumpedDir.parse_markdown, dumps)\n",
    "dumps = list(dumps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "600d3657",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dumps:\n",
    "    d.dump_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45edd199",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
