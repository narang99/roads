# Synthetic data: T2

- We need to generate better synthetic data.  
- I'm going all in on faithfully recreating a garbage scene on another clean photo, using a reference photo.  
- There would be different stuff for different kinds of garbage and scenery.  
- Currently there are two
  - A straight road, garbage along the edges. This puts perspective and scaling into play
  - A simple garbage in front of your face, not long going image.  
- I've also got a better method of extracting garbage fragments, I used k-means clustering now. It is useful in finding garbage which is very similar to background also. Amazing
- While taking out fragments, we now need to store metadata. 
  - What kind of image it is.  
  - What is the scale of the garbage (perspective based height).  
  - Where it is placed might be important too.  



## Generation roadmap
- I'm first focusing on garbage in front of you images. Taking out fragments and put it in some other place to see if we can recreate it.  
- First do it manually.  
- Points which can be useful
  - Where the garbage is placed (road? ground? grass?)
  - What kind of image it is


- Extract fragments.  
- Manually place each fragment somewhere to see what the problems are first.  

- Each fragment is a directory now.
  - it has `image.png`
  - contains a JSON file `meta.json`

- Each image is also a directory (name id)
  - `image.jpg` or `image.png`
  - exported JSON for that image
  - `seg.json` the segmask of that image using segformer (for getting background)
    - this will be done later

- For basic experimentation, I need simpler terms. For now, I have some fragments, and i try to put each in a fresh image to see how it works

Crop extraction pipeline
- Take a JSON file from label studio which contains absolute paths to image and its corresponding annotations
- for each image
  - generate a crop, give the crop an id, push to a directory of crops
  - dump the raw crop `raw.png`
  - dump `meta.json` containing the start point of the crop along with the height and width of the original image
    - we can later add more metadata
  - the user then goes through all the crops and creates `proc.png`


# obs

- i need to find the perspective line from which i got the object. I then need to place the object along a similar perspective line in the target photo, if it exists 
- the size is also important, the object needs scaling
- now i need to find the "perspective" line and some sort of way to find similarity
- and the object scaling size along a given line. I will 
- it looks slightly more realistic now. I need to now investigate how it looks zoomed in

- Now looking at it zoomed in, I see that it has black weird edges. Its not blending in. The original image is quite blended in. This is a problem of the processed threshold having black background (kmeans did not remove it correctly). I could do manual thresholding for all images (this is not very useful when its not a white fragment now)
- The black edges are coming from resizing, changed interpolation to internearest and it got better. I'm satisfied with how the fragment is looking now.


- Next steps:
  - Store the horizon point in fragment's original image
  - Somehow represent the perspective line (If i have vanishing point, its simply a line with a slope, I should be fine with that i think)
  - Given new vanishing point in target image, it should be easy to find the x where i can place an image for a given y

- Note that we are ignoring images without vanishing points right now. Only optimising for vanishing points
- I can label the perspective lines myself in label studio