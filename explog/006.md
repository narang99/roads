# Segmentation of small objects

this is actually not quite bad. Although segmentation takes a lot of compute and thus its hard for me to experiment a lot.  
It might be better to setup a simple experiment notebook which keeps running my experiments. Colab takes too long for testing though.  

I made a mistake by not checking my extraction code for data synthesis before training the model. From now on, always create functions to verify how the data looks after extraction. 


The segmnetaiotn mask is better than other models
Some observations

- I started by tiling an image to the size the model expects, passing the tiles directly to the model, and then reconstructing the image from the output. This gave OK performance. 
- It was missing some obvious garbage pieces and didn’t work on pieces larger than 50 × 50. The 50 × 50 tile also looked different from the training images. 
- I took a crop that contained garbage, and it didn't work with our default algorithm. Then I applied a padded resize during the preprocessing step we use for all our training data, and it started giving good results. 
- It’s useful to create boxes for each crop in the prediction function so we can analyze the model better. I also need to review all missed data points to see how the model performs on each crop individually. 
- The model also doesn’t work well when the images are padded to preserve aspect ratio. I assume this is because the inputs weren’t padded at all; they were perfectly sized. 
- The training data had all objects centered in the garbage photos. I need to augment the data to place each object in different corners. it works well when i center a piece of garbage explicitly
- It has worked on 2 garbage pieces centered also