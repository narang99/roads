# fastai classifier on small cuts

Instead of making a big YOLO model now, I'm going to detecting whether a 50x50 pixel cut of an image contains garbage.  

We use the TACO dataset for training. There is code to extract a crop out of the dataset. We shrink images to make it similar to real world data.  

## 001

First attempt was better than other older attempts.  
The model did okay enough in finding some new garbage that it had never seen before.   
I'm using one of the images in my own garbage dataset for testing.  

- Ran only 10 cycles, without unfreezing head, without augmentations.  

The experiment depends on:
- Number of total images extracted
- Augmentations
- Model architecture
- How well it performs in real world (this might be my test split?)
- image size / cut size

I need to first form a baseline. I also need a test set.  

### 01
Baseline
- no augmentations
- 5000 images
- mobilenet_v2
- size: 50
- Loss fn: none

I had increased data from 1000 to 5000, the results are much better on the test image.   

![Image](../datasets/T004-taco-crops/001/res.png)

```
[0, 0.6704698801040649, 0.8745170831680298, '00:06']
[0, 0.46069100499153137, 0.9539707899093628, '00:08']
[1, 0.3692852854728699, 0.6587792038917542, '00:08']
[2, 0.3423748314380646, 0.5984047651290894, '00:07']
[3, 0.30394527316093445, 0.43964460492134094, '00:06']
[4, 0.27040088176727295, 0.3391488194465637, '00:08']
[5, 0.23712214827537537, 0.2532862722873688, '00:08']
[6, 0.22443599998950958, 0.2037893831729889, '00:08']
[7, 0.18859390914440155, 0.20494478940963745, '00:09']
[8, 0.18157190084457397, 0.2007972151041031, '00:09']
[9, 0.1785021275281906, 0.19957658648490906, '00:07']
```

I forgot to save the model ;_;

### 02

- Changed dataset size to 10000

No substantial improvement. I see some overfitting I think

```
[0, 0.524621844291687, 0.7264582514762878, '00:18']
[0, 0.3369419574737549, 0.6069709658622742, '00:14']
[1, 0.3027571737766266, 0.32461223006248474, '00:13']
[2, 0.2642553150653839, 0.23376184701919556, '00:13']
[3, 0.23243741691112518, 0.21405450999736786, '00:14']
[4, 0.1960383802652359, 0.20458057522773743, '00:14']
[5, 0.18665564060211182, 0.18913228809833527, '00:13']
[6, 0.16048960387706757, 0.18070441484451294, '00:13']
[7, 0.1523756980895996, 0.17838405072689056, '00:13']
[8, 0.141083225607872, 0.17608077824115753, '00:13']
[9, 0.13121774792671204, 0.17633019387722015, '00:13']
```

![Image](../datasets/T004-taco-crops/002/res.png)

### 03

Add augmentations. Make dataset 5000 again.  
Needed more epochs, loss has not converged early.  

Not sure, don't see a lot of changes, although there seem to be more boxes.  
Loss did stabilise, its not as good as the first experiment though.  

```
[0, 0.7951254844665527, 1.4611352682113647, '00:10']
[0, 0.6214095950126648, 0.7896298170089722, '00:10']
[1, 0.552376389503479, 0.6112728714942932, '00:09']
[2, 0.5249661803245544, 0.6053832769393921, '00:09']
[3, 0.4883546829223633, 0.41702011227607727, '00:09']
[4, 0.46568986773490906, 0.3512972295284271, '00:09']
[5, 0.42870986461639404, 0.3104511797428131, '00:08']
[6, 0.420382559299469, 0.2920598089694977, '00:12']
[7, 0.39832955598831177, 0.2816503643989563, '00:09']
[8, 0.37351685762405396, 0.2680111825466156, '00:14']
[9, 0.35895276069641113, 0.2669823467731476, '00:09']
[10, 0.332695335149765, 0.2588888108730316, '00:09']
[11, 0.3344603478908539, 0.2531983554363251, '00:13']
[12, 0.31862401962280273, 0.25287869572639465, '00:16']
[13, 0.3172438144683838, 0.2406296283006668, '00:13']
[14, 0.3119004964828491, 0.24073678255081177, '00:12']
[15, 0.3024502098560333, 0.23540692031383514, '00:11']
[16, 0.31818249821662903, 0.23522883653640747, '00:10']
[17, 0.30483153462409973, 0.23198546469211578, '00:14']
[18, 0.29611343145370483, 0.23013365268707275, '00:19']
[19, 0.30064064264297485, 0.2301216423511505, '00:15']
```

![Image](../datasets/T004-taco-crops/003/res.png)

This model has similar or worse performance than the first model I feel. Lets increase the training data and use more aggressive augmentationsgTg